{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4c8cfe",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811d85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean this up\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm as notebook_tqdm\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from nltk import pos_tag, download\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from math import ceil\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861d239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the Hugging Face Hub\n",
    "notebook_tqdm.tqdm.pandas()\n",
    "dataset = load_dataset('ucberkeley-dlab/measuring-hate-speech')\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df_raw = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947cbb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8 # TODO: factor this out everywhere\n",
    "TEST_RATIO = 1 - TRAIN_RATIO\n",
    "HATE_SPEECH_THRESHOLD_UNNORMALIZED = 0.5\n",
    "SUPPORTIVE_THRESHOLD_UNNORMALIZED = -1\n",
    "EVALUATION_METRICS = ['precision', 'recall', 'f1-score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a25d9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1f6ab",
   "metadata": {},
   "source": [
    "# 2. Filtering & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b536b",
   "metadata": {},
   "source": [
    "### 2.0. Function definitions & variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1399e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hate_speech_score_histogram(df: pd.DataFrame, hate_threshold: float, supportive_threshold: float):\n",
    "    \"\"\"\n",
    "    Plots a histogram of the hate_speech_score with annotated lines at the thresholds\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to plot the histogram from\n",
    "        hate_threshold (float): The threshold for hate speech\n",
    "        supportive_threshold (float): The threshold for supportive speech\n",
    "\n",
    "    Returns:\n",
    "        None (plots the histogram)\n",
    "    \"\"\"\n",
    "\n",
    "    total_count = df.shape[0]\n",
    "    \n",
    "    # Calculate the amount of values below the supportive threshold\n",
    "    supportive_count = df[df['hate_speech_score'] < supportive_threshold].shape[0]\n",
    "\n",
    "    # Calculate the amount of values above the hate threshold\n",
    "    hate_count = df[df['hate_speech_score'] > hate_threshold].shape[0]\n",
    "\n",
    "    # Calculate the amount of values between the thresholds\n",
    "    neutral_count = df[(df['hate_speech_score'] >= supportive_threshold) & (df['hate_speech_score'] <= hate_threshold)].shape[0]\n",
    "\n",
    "    print(f\"Total count: {total_count}\")\n",
    "    print(f\"Supportive count: {supportive_count} ({supportive_count / total_count * 100:.2f}%)\")\n",
    "    print(f\"Hate count: {hate_count} ({hate_count / total_count * 100:.2f}%)\")\n",
    "    print(f\"Neutral count: {neutral_count} ({neutral_count / total_count * 100:.2f}%)\")\n",
    "\n",
    "    # Plot in histogram as well\n",
    "    plt.hist(df['hate_speech_score'], bins=100)\n",
    "    plt.axvline(hate_threshold, color='red', linestyle='--', label='Hate Speech Threshold')\n",
    "    plt.axvline(supportive_threshold, color='blue', linestyle='--', label='Supportive Threshold')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_target_columns_distribution(df: pd.DataFrame, target_cols: list[str]):\n",
    "    \"\"\"\n",
    "    Plots a barchart of the average value of the target columns for the hate speech observations\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to plot the distribution from\n",
    "        target_cols (list[str]): The columns to plot\n",
    "\n",
    "    Returns:\n",
    "        None (plots the distribution)\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter to count hate speech only\n",
    "    hate_speech_df = df[df['is_hate_speech'] == 1]\n",
    "\n",
    "    # Calculate the mean value of the target columns for the hate speech observations and sort them descending\n",
    "    mean_values = hate_speech_df[target_cols].mean()\n",
    "    mean_values = mean_values.sort_values(ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    mean_values.plot(kind='bar')\n",
    "    plt.title('Percentage of hate speech columns targeting a specific group')\n",
    "    plt.xlabel('Target Column')\n",
    "    plt.ylabel('Percentage of hate speech')\n",
    "    plt.show()\n",
    "\n",
    "def plot_target_columns_detailed(df: pd.DataFrame, target_cols: list[str], fig_size: tuple[int, int] = (15, 30), y_max: float = 0.35):\n",
    "    \"\"\"\n",
    "    Plots a bar chart for each of the target columns with detailed breakdowns of sub-groups\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to plot the distribution from\n",
    "        target_cols (list[str]): The columns to plot\n",
    "\n",
    "    Returns:\n",
    "        None (plots the distributions)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter to count hate speech only\n",
    "    hate_speech_df = df[df['is_hate_speech'] == 1]\n",
    "\n",
    "    # initalize axes to plot the 7 sub-groups in two rows of 3 and a last row of 1\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=2, figsize=fig_size)\n",
    "    # For each of the target columns, make a list of means of of hate speach targeting its sub-groups\n",
    "    for ax, col in zip(axes.flatten(), target_cols):\n",
    "\n",
    "        # Calculate the number of observations with a non-zero value for the target column\n",
    "        non_zero_count = hate_speech_df[col].value_counts()[1]\n",
    "\n",
    "        # identify all columns in the dataframe that starts with the target column name\n",
    "        sub_groups_column_names = [column for column in df.columns if column.startswith(col)]\n",
    "\n",
    "        # remove the original target column from the list\n",
    "        sub_groups_column_names.remove(col)\n",
    "        col_name = col.replace(\"target_\", \"\").title()\n",
    "\n",
    "        # calculate the mean values for each sub-group\n",
    "        sub_group_means = hate_speech_df[sub_groups_column_names].mean()\n",
    "        sub_group_means = sub_group_means.sort_values(ascending=False)\n",
    "\n",
    "        # remove the col string from the labels\n",
    "        labels = sub_group_means.index.str.replace((col + \"_\"), '')\n",
    "\n",
    "        # plot on the specific axis\n",
    "        sub_group_means.plot(kind='bar', ax=ax)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set_title(f'Percentage of hate speech targeting {col_name} by sub-group \\n(n={non_zero_count})')\n",
    "        ax.set_xlabel('Sub-group')\n",
    "        ax.set_ylabel('Percentage of hate speech')\n",
    "        ax.set_ylim(0, y_max)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.7)\n",
    "\n",
    "def inspect_for_cor(df, columns: list[str], plot_label: str, hate_speech_only: bool = True):\n",
    "\n",
    "    # filter for hate speech only if specified\n",
    "    if hate_speech_only:\n",
    "        df = df[df['is_hate_speech'] == 1]\n",
    "    \n",
    "    # Make a correlation matrix\n",
    "    corr_matrix = df[columns].corr()\n",
    "\n",
    "    # Plot the correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title(f'Correlation Heatmap for {plot_label}')\n",
    "    plt.show()\n",
    "\n",
    "    # Make a pairplot of the columns\n",
    "    sns.pairplot(df[columns])\n",
    "    plt.title(f'Pairplot for {plot_label}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22853d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_cols = [\n",
    "    'insult',\n",
    "    'humiliate',\n",
    "    'status',\n",
    "    'dehumanize',\n",
    "    'violence',\n",
    "    'genocide'\n",
    "]\n",
    "\n",
    "target_cols_lvl1_and_2 = [col for col in df_raw.columns if col.startswith('target_')]\n",
    "\n",
    "target_cols_lvl1 = [\n",
    "    'target_race',\n",
    "    'target_religion',\n",
    "    'target_origin',\n",
    "    'target_gender',\n",
    "    'target_sexuality',\n",
    "    'target_age',\n",
    "    'target_disability',    \n",
    "]\n",
    "\n",
    "cols_to_keep = [\n",
    "    'comment_id', \n",
    "    'text', \n",
    "    'hate_speech_score',\n",
    "    *aspect_cols,\n",
    "    *target_cols_lvl1_and_2\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37613ea6",
   "metadata": {},
   "source": [
    "### 2.1. Pre-filtering EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91890c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hate_speech_score_histogram(df_raw, HATE_SPEECH_THRESHOLD_UNNORMALIZED, SUPPORTIVE_THRESHOLD_UNNORMALIZED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f3881",
   "metadata": {},
   "source": [
    "### 2.2. Filtering & reformating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84c761",
   "metadata": {},
   "source": [
    "TODO: Make list of filtering actions for report \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows based on the 'comment_id' column\n",
    "duplicate_texts = df_raw[df_raw.duplicated(subset='comment_id', keep=False)]\n",
    "print(f\"Number of duplicate texts: {duplicate_texts.shape[0]}\")\n",
    "duplicate_texts[['comment_id', 'text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dc8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'comment_id' and count unique 'hate_speech_score' values per group\n",
    "score_check = df_raw.groupby(\"comment_id\")[\"hate_speech_score\"].nunique()\n",
    "\n",
    "# Find texts with more than one unique score\n",
    "inconsistent = score_check[score_check > 1]\n",
    "\n",
    "# Show how many inconsistencies there are\n",
    "print(f\"Number of 'comment_id' entries with inconsistent scores: {len(inconsistent)}\")\n",
    "\n",
    "# Optionally, view a few examples\n",
    "if not inconsistent.empty:\n",
    "    print(df_raw[df_raw[\"comment_id\"].isin(inconsistent.index)].sort_values(\"comment_id\").head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe2e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select relevant columns\n",
    "df_relevant = df_raw[cols_to_keep].copy()\n",
    "\n",
    "# Step 2: Convert booleans to integers for mean calculation\n",
    "bool_cols = [col for col in df_relevant.columns if col.startswith('target_')]\n",
    "df_relevant[bool_cols] = df_relevant[bool_cols].astype(int)\n",
    "\n",
    "# Step 3: Group by 'comment_id' and 'text' to keep them in final output\n",
    "filtered_df = df_relevant.groupby(['comment_id', 'text']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = filtered_df['hate_speech_score'].min()  # e.g., -8.34\n",
    "max_score = filtered_df['hate_speech_score'].max()  # e.g., +6.3\n",
    "\n",
    "# replace hate speech core column with normalized column - [0, 1]\n",
    "filtered_df['hate_speech_score'] = (filtered_df['hate_speech_score'] - min_score) / (max_score - min_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f321de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the thresholds given by the dataset authors\n",
    "HATE_SPEECH_THRESHOLD_NORMALIZED = (0.5 - min_score) / (max_score - min_score)  \n",
    "SUPPORTIVE_THRESHOLD_NORMALIZED = (-1 - min_score) / (max_score - min_score)\n",
    "\n",
    "# Add binary columns for hate speech\n",
    "filtered_df.insert(filtered_df.columns.get_loc('hate_speech_score') + 1, 'is_hate_speech', (filtered_df['hate_speech_score'] > HATE_SPEECH_THRESHOLD_NORMALIZED).astype(int))\n",
    "\n",
    "# Print new normalized thresholds \n",
    "print(f\"Threshold for hate speech: {HATE_SPEECH_THRESHOLD_NORMALIZED:.3f} -> Equivalent to {0.5} on the original scale\")\n",
    "print(f\"Threshold for counter speech: {SUPPORTIVE_THRESHOLD_NORMALIZED:.3f} -> Equivalent to {-1} on the original scale\")\n",
    "print(f\"Between the two thresholds: {HATE_SPEECH_THRESHOLD_NORMALIZED:.3f} and {SUPPORTIVE_THRESHOLD_NORMALIZED:.3f} -> Equivalent to {0.5} and {-1} on the original scale which is the unambiguous region\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34e919",
   "metadata": {},
   "source": [
    "### 2.3. Post-filtering EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35914d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hate_speech_score_histogram(filtered_df, HATE_SPEECH_THRESHOLD_NORMALIZED, SUPPORTIVE_THRESHOLD_NORMALIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_target_columns_distribution(filtered_df, target_cols_lvl1)\n",
    "plot_target_columns_detailed(filtered_df, target_cols_lvl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_for_cor(filtered_df, target_cols_lvl1, 'Target Columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dc0b7c",
   "metadata": {},
   "source": [
    "# 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a48fc",
   "metadata": {},
   "source": [
    "### 3.0 Function & Class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bbe327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_results(results: list, report: dict, model_name: str, metrics: list[str]):\n",
    "    \n",
    "    append_dict = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        append_dict[f'{metric}_hate'] = report['1'][metric]\n",
    "        append_dict[f'{metric}_non_hate'] = report['0'][metric]\n",
    "        append_dict[f'{metric}_macro_avg'] = report['macro avg'][metric]\n",
    "        append_dict[f'{metric}_weighted_avg'] = report['weighted avg'][metric]\n",
    "\n",
    "    append_dict['accuracy'] = report['accuracy']\n",
    "\n",
    "    results.append({\n",
    "        'model': model_name,\n",
    "        **append_dict\n",
    "    })\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"\n",
    "    Convert the part of speech tag to a format that WordNet lemmatizer can understand.\n",
    "    starts with 'J' for adjectives, 'V' for verbs, 'N' for nouns, and 'R' for adverbs.\n",
    "    Args:\n",
    "        tag (str): The part of speech tag.\n",
    "    Returns:\n",
    "        str: The WordNet part of speech tag.\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def is_ascii(token):\n",
    "    return all(ord(c) < 128 for c in token)\n",
    "\n",
    "def remove_repeated_chars(token, threshold=3):\n",
    "    return re.sub(r'(.)\\1{' + str(threshold) + r',}', r'\\1', token)\n",
    "\n",
    "def bow_preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by tokenizing, removing punctuation, stop words,\n",
    "    and lemmatizing the words.\n",
    "    Args:\n",
    "        text (str): The text to preprocess.\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    Example:\n",
    "        >>> preprocess(\"This is an EXAMPLE sentence!!!.\")\n",
    "        'example sentence'\n",
    "        \n",
    "    \"\"\"\n",
    "    tokens = tknzr.tokenize(text) # Tokenize the text\n",
    "    tokens = [word.lower() for word in tokens if word not in string.punctuation] # Remove punctuation\n",
    "    tokens = [word for word in tokens if word not in stop_words] # Remove stop words\n",
    "    tokens = [remove_repeated_chars(word) for word in tokens] # Remove repeated characters, ex: \"loooove\" -> \"love\"\n",
    "    tokens = [word for word in tokens if len(word) >= 2 and is_ascii(word)]  # Filter by length and ASCII\n",
    "    pos_tags = pos_tag(tokens) # Get part of speech tags\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags] # Lemmatize the words using the part of speech tags\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "def stream_batches(texts, labels, batch_size=32, max_length=256):\n",
    "    \"\"\"\n",
    "    Streams batches into SGD classifier\n",
    "\n",
    "    Args:\n",
    "        texts (pd.Series): The texts to stream\n",
    "        labels (pd.Series): The labels to stream\n",
    "        batch_size (int): The batch size\n",
    "        max_length (int): The maximum length of the texts\n",
    "    \n",
    "    Returns:\n",
    "        generator: A generator of batches\n",
    "    \"\"\"\n",
    "    n = len(texts)\n",
    "    n_batches = ceil(n / batch_size)\n",
    "\n",
    "    # Iterate over the batches to yield them one by one\n",
    "    for i in range(n_batches):\n",
    "        batch_texts = texts.iloc[i*batch_size:(i+1)*batch_size].tolist()\n",
    "        batch_labels = labels.iloc[i*batch_size:(i+1)*batch_size].values\n",
    "        \n",
    "        # dynamic padding + truncation\n",
    "        enc = bert_tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        yield enc[\"input_ids\"], enc[\"attention_mask\"], batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5db4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWTextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.progress_apply(bow_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a64241",
   "metadata": {},
   "source": [
    "### 3.1. BoW + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for Bag of Words (BoW) representation\n",
    "bow_df = filtered_df.copy()\n",
    "\n",
    "# Releveant columns\n",
    "bow_cols = ['text', 'is_hate_speech']\n",
    "\n",
    "# Select relevant columns\n",
    "bow_df = bow_df[bow_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc79ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = bow_df['is_hate_speech'].value_counts()\n",
    "label_percentage = bow_df['is_hate_speech'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Label Counts:\")\n",
    "print(label_counts)\n",
    "print(\"\\nLabel Percentage Distribution:\")\n",
    "print(label_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36de96e",
   "metadata": {},
   "source": [
    "##### 3.1.1 Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb1c6f",
   "metadata": {},
   "source": [
    "TODO: Maybe remove preprocessing from pipepine to just have it done once for \"Part 2\" of Assignment 3 approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76203444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "for resource in ['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger_eng']:\n",
    "    nltk.download(resource, quiet=True)\n",
    "\n",
    "# Tokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_pipeline = Pipeline([\n",
    "    ('preprocessor', BOWTextPreprocessor()),\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression(class_weight='balanced'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bow_df['text']\n",
    "y = bow_df['is_hate_speech']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_RATIO, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264239dd",
   "metadata": {},
   "source": [
    "##### 3.1.2. Fit model to training set - BoW + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to the training data\n",
    "bow_pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866b3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_bow = bow_pipeline.predict(X_test)\n",
    "\n",
    "# Generate the classification report\n",
    "report_bow = classification_report(y_test, y_pred_bow, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31455156",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_results(results, report_bow, 'BoW + LogisticRegression', EVALUATION_METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa679c",
   "metadata": {},
   "source": [
    "### 3.2 TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_tfidf = Pipeline([\n",
    "    ('preprocessor', BOWTextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', LogisticRegression(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_tfidf.fit(X_train, y_train)\n",
    "y_pred_tfidf = pipeline_tfidf.predict(X_test)\n",
    "\n",
    "report_tfidf = classification_report(y_test, y_pred_tfidf, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aeb93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_results(results, report_tfidf, 'TF-IDF + LogisticRegression', EVALUATION_METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb087e",
   "metadata": {},
   "source": [
    "##### 3.2.2 Hyperparameter tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a76abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_tuned = Pipeline([\n",
    "    ('preprocessor', BOWTextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer()),  # or CountVectorizer()\n",
    "    ('classifier', LogisticRegression(class_weight='balanced', max_iter=1000))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1,1), (1,2)],\n",
    "    'vectorizer__max_features': [5000, 10000],\n",
    "    'vectorizer__max_df': [0.75, 0.9],\n",
    "    'classifier__C': [0.1, 1],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear']  # Needed to compare L1 and L2\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline_tuned,\n",
    "    param_grid,\n",
    "    cv=3, \n",
    "    scoring='f1', \n",
    "    verbose=2, \n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcadc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best cross-validated F1 score:\", grid_search.best_score_)\n",
    "\n",
    "# Use best model to predict test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "\n",
    "report_tuned = classification_report(y_test, y_pred_tuned, output_dict=True)\n",
    "\n",
    "append_results(results, report_tuned, 'Tuned BoW + LogisticRegression', EVALUATION_METRICS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb46646",
   "metadata": {},
   "source": [
    "### 3.3. BERT + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc28eb1",
   "metadata": {},
   "source": [
    "##### 3.3.1. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23987cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "bert_tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c7a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df = filtered_df.copy()\n",
    "\n",
    "# Optional: Take a subset to ease computational load\n",
    "bert_df = bert_df.sample(frac=1)\n",
    "\n",
    "X = bert_df['text']\n",
    "y = bert_df['is_hate_speech']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_RATIO, random_state=42)\n",
    "\n",
    "batch_size = 16\n",
    "num_batches = ceil(len(X_train) / batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802848f2",
   "metadata": {},
   "source": [
    "Computing class weights for the balance_weight argument in partial fit (TODO: remove this and \"class weight\" argument from model instantiation if we decide to do under/oversampling instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025733fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes must match what you‚Äôll pass to partial_fit\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "# Compute: this returns an array [w_for_0, w_for_1]\n",
    "weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train,             # your full training-label vector\n",
    ")\n",
    "\n",
    "# Turn it into a dict: { class_label: weight, ‚Ä¶ }\n",
    "class_weight_dict = {c: w for c, w in zip(classes, weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0189b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SGD-based logistic regression\n",
    "clf = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", max_iter=1, warm_start=True, class_weight=class_weight_dict)\n",
    "classes = [0, 1]          \n",
    "first_batch = True\n",
    "\n",
    "train_iter = tqdm(\n",
    "    stream_batches(X_train, y_train, batch_size=batch_size),\n",
    "    total=num_batches,\n",
    "    desc=\"Training\"\n",
    ")\n",
    "\n",
    "\n",
    "# Loop (stream) through training data\n",
    "first_batch = True\n",
    "for input_ids, attn_mask, y_batch in train_iter:\n",
    "\n",
    "    # Get the features from the model\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids, attention_mask=attn_mask)\n",
    "\n",
    "    # pull out the [CLS] token embedding for each example\n",
    "    feats = out.last_hidden_state[:,0,:].cpu().numpy()\n",
    "    \n",
    "    if first_batch:\n",
    "        # partial_fit needs to see the 'classes' array at first call\n",
    "        clf.partial_fit(feats, y_batch, classes=classes)\n",
    "        first_batch = False\n",
    "    else:\n",
    "        clf.partial_fit(feats, y_batch)\n",
    "    \n",
    "\n",
    "# Evaluate on tast data, with streaming\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test data\n",
    "for input_ids, attn_mask, y_batch in stream_batches(X_test, y_test, batch_size=16):\n",
    "\n",
    "    # Get the features from the model\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids, attention_mask=attn_mask)\n",
    "    feats = out.last_hidden_state[:,0,:].cpu().numpy()\n",
    "\n",
    "    # Predict the labels\n",
    "    preds = clf.predict(feats)\n",
    "    \n",
    "    # Save\n",
    "    all_preds.append(preds)\n",
    "    all_labels.append(y_batch)\n",
    "\n",
    "# flatten and compute metric\n",
    "y_true = np.concatenate(all_labels)\n",
    "y_pred = np.concatenate(all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ef982",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_distilbert = classification_report(y_true, y_pred, output_dict=True)\n",
    "append_results(results, report_distilbert, 'BERT + LogisticRegression', EVALUATION_METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33600f62",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45021dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4682a",
   "metadata": {},
   "source": [
    "# 5. LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4409057",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c73c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_df = filtered_df[['text', 'is_hate_speech']].copy()\n",
    "\n",
    "llm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe54811",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(llm_df, test_size=0.2, random_state=42, stratify=llm_df.is_hate_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e08671",
   "metadata": {},
   "source": [
    "## 5.1  Setting up the prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b7567",
   "metadata": {},
   "source": [
    "### 5.1.1 Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3222a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_template = (\n",
    "    \"Decide whether the following text is hate speech. \"\n",
    "    \"Respond only with ‚Äúyes‚Äù or ‚Äúno‚Äù.\\n\\n\"\n",
    "    \"Text: \\\"{text}\\\"\\n\"\n",
    "    \"Hate speech?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477287d1",
   "metadata": {},
   "source": [
    "### 5.1.2 Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd59a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = [\n",
    "    (\"I love you all\", \"no\"),\n",
    "    (\"We should kill them\", \"yes\"),\n",
    "    (\"That group is weird\", \"yes\")\n",
    "]\n",
    "\n",
    "few_shot_template = \"Decide whether each of the following texts is hate speech (yes/no).\\n\\n\"\n",
    "for txt, label in few_shot_examples:\n",
    "    few_shot_template += f\"Text: \\\"{txt}\\\"\\nLabel: {label}\\n\\n\"\n",
    "few_shot_template += \"Text: \\\"{text}\\\"\\nLabel:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85090e8e",
   "metadata": {},
   "source": [
    "### 5.1.3 Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fecc308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_template = (\n",
    "    \"Here is a text. Think step by step about whether it qualifies as hate speech. \"\n",
    "    \"After your reasoning, give a final answer ‚Äúyes‚Äù or ‚Äúno‚Äù.\\n\\n\"\n",
    "    \"Text: \\\"{text}\\\"\\n\\n\"\n",
    "    \"Reasoning:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78281e7f",
   "metadata": {},
   "source": [
    "## 5.2 Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56897b13",
   "metadata": {},
   "source": [
    "### 5.2.1 OpenAI - gpt-4.1-nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf4243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key from the .env file\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI client with the API key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def _classify_openai(prompt: str) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano-2025-04-14\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        max_completion_tokens=60\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3933f17",
   "metadata": {},
   "source": [
    "### 5.2.2 DeepSeek - R1 Distill Qwen 1.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bd39a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "# ‚Äî DeepSeek-R1 (Hugging Face) setup ‚Äî\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer_ds = AutoTokenizer.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model_ds = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "\n",
    "def _classify_deepseek(prompt: str) -> str:\n",
    "    inputs  = tokenizer_ds(prompt, return_tensors=\"pt\").to(model_ds.device)\n",
    "    outputs = model_ds.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=60,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer_ds.eos_token_id,\n",
    "    )\n",
    "    gen     = tokenizer_ds.decode(\n",
    "        outputs[0][ inputs.input_ids.shape[-1] : ],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return gen.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9cb384",
   "metadata": {},
   "source": [
    "### 5.2.3 xAI - Grok 3 mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "996dca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "grok_client = OpenAI(\n",
    "  api_key = os.getenv(\"GROK_API_KEY\"),\n",
    "  base_url=\"https://api.x.ai/v1\",\n",
    ")\n",
    "\n",
    "def _classify_grok(prompt: str) -> str:\n",
    "    resp = grok_client.chat.completions.create(\n",
    "        model=\"grok-3-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        max_completion_tokens=60\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2e7b6",
   "metadata": {},
   "source": [
    "## 5.3 Choose the model you want to continue with\n",
    "\n",
    "üö® **Do so by changing the LLM_BACKEND variable underneath** üö®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a1b00b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_BACKEND = \"openai\"   # or \"deepseek\" or \"grok\"\n",
    "\n",
    "def classify_with_llm(prompt: str) -> str:\n",
    "    if LLM_BACKEND == \"openai\":\n",
    "        return _classify_openai(prompt)\n",
    "    elif LLM_BACKEND == \"deepseek\":\n",
    "        return _classify_deepseek(prompt)\n",
    "    elif LLM_BACKEND == \"grok\":\n",
    "        return _classify_grok(prompt)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown LLM backend: {LLM_BACKEND}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c21fe0e",
   "metadata": {},
   "source": [
    "## 5.4 Individual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7dc79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_comment(text: str, method: str = \"simple\") -> str:\n",
    "    \"\"\"\n",
    "    Classify a single comment as hate speech (‚Äúyes‚Äù/‚Äúno‚Äù) using one of:\n",
    "      ‚Äì ‚Äúsimple‚Äù : simple prompting\n",
    "      ‚Äì ‚Äúfew‚Äù    : few-shot prompting\n",
    "      ‚Äì ‚Äúcot‚Äù    : chain-of-thought prompting\n",
    "    \"\"\"\n",
    "    if method == \"simple\":\n",
    "        template = simple_template\n",
    "    elif method == \"few\":\n",
    "        template = few_shot_template\n",
    "    elif method == \"cot\":\n",
    "        template = cot_template\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    prompt = template.format(text=text)\n",
    "    out    = classify_with_llm(prompt)\n",
    "    return \"yes\" if \"yes\" in out else \"no\"\n",
    "\n",
    "def test_comment():\n",
    "    \"\"\"\n",
    "    Prompt the user for a comment, echo it, and show all three classifications.\n",
    "    \"\"\"\n",
    "    comment = input(\"Enter a comment to classify: \")\n",
    "    print(f\"\\nComment to classify: \\\"{comment}\\\"\\n\")\n",
    "\n",
    "    for method, label in [\n",
    "        (\"simple\", \"Simple prompting\"),\n",
    "        (\"few\",    \"Few-shot prompting\"),\n",
    "        (\"cot\",    \"Chain-of-Thought prompting\")\n",
    "    ]:\n",
    "        pred = classify_comment(comment, method=method)\n",
    "        print(f\"{label:25}: {pred}\")\n",
    "\n",
    "import time\n",
    "\n",
    "def test_comment_timed():\n",
    "    \"\"\"\n",
    "    Prompt the user for a comment, echo it, then for each method:\n",
    "    ‚Äì time how long classify_comment takes\n",
    "    ‚Äì print both the prediction and the time elapsed\n",
    "    \"\"\"\n",
    "    comment = input(\"Enter a comment to classify: \")\n",
    "    print(f\"\\nComment to classify: \\\"{comment}\\\"\\n\")\n",
    "\n",
    "    for method, label in [\n",
    "        (\"simple\", \"Simple prompting\"),\n",
    "        (\"few\",    \"Few-shot prompting\"),\n",
    "        (\"cot\",    \"Chain-of-Thought prompting\")\n",
    "    ]:\n",
    "        start = time.perf_counter()\n",
    "        pred  = classify_comment(comment, method=method)\n",
    "        end   = time.perf_counter()\n",
    "        elapsed = end - start\n",
    "        print(f\"{label:25}: {pred:<3}  (took {elapsed:.2f} s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad62359",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment_timed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a9705",
   "metadata": {},
   "source": [
    "## 5.5 Run classification on larger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 20 random rows (or fewer if test_df has <20 rows)\n",
    "sample_df = test_df.sample(\n",
    "    n=min(20, len(test_df)),\n",
    "    random_state=42,        # for reproducibility\n",
    ")\n",
    "\n",
    "def run_classification(df, template_fn, classify_fn):\n",
    "    preds = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        prompt = template_fn.format(text=row.text)\n",
    "        out    = classify_fn(prompt)\n",
    "        preds.append(\"yes\" if \"yes\" in out else \"no\")\n",
    "    return np.array(preds)\n",
    "\n",
    "# Example: assuming `sample_df` is defined\n",
    "simple_preds   = run_classification(sample_df, simple_template, classify_with_llm)\n",
    "few_shot_preds = run_classification(sample_df, few_shot_template, classify_with_llm)\n",
    "cot_preds      = run_classification(sample_df, cot_template, classify_with_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63af9ef",
   "metadata": {},
   "source": [
    "## 5.6 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64186d4b",
   "metadata": {},
   "source": [
    "TODO: Get same metrics as for the first couple of models and add the name of the model (ideally, just make it work with the other function further up in the notebook used for the rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f85a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "for name, preds in [(\"Simple\", simple_preds),\n",
    "                    (\"Few-Shot\", few_shot_preds),\n",
    "                    (\"Chain-of-Thought\", cot_preds)]:\n",
    "    acc = accuracy_score(sample_df.is_hate_speech.map({1:\"yes\",0:\"no\"}), preds)\n",
    "    f1 = f1_score(sample_df.is_hate_speech.map({1:\"yes\",0:\"no\"}), preds, pos_label=\"yes\")\n",
    "    print(f\"{name:20}  Acc: {acc:.3f}  F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7558ecc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
