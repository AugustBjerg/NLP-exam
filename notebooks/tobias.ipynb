{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4c8cfe",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "811d85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean this up\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm as notebook_tqdm\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from nltk import pos_tag, download\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from math import ceil\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "861d239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the Hugging Face Hub\n",
    "notebook_tqdm.tqdm.pandas()\n",
    "dataset = load_dataset('ucberkeley-dlab/measuring-hate-speech')\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df_raw = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "947cbb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8 # TODO: factor this out everywhere\n",
    "TEST_RATIO = 1 - TRAIN_RATIO\n",
    "HATE_SPEECH_THRESHOLD_UNNORMALIZED = 0.5\n",
    "SUPPORTIVE_THRESHOLD_UNNORMALIZED = -1\n",
    "EVALUATION_METRICS = ['precision', 'recall', 'f1-score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a25d9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1f6ab",
   "metadata": {},
   "source": [
    "# 2. Filtering & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b536b",
   "metadata": {},
   "source": [
    "### 2.0. Function definitions & variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1399e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hate_speech_score_histogram(df: pd.DataFrame, hate_threshold: float, supportive_threshold: float):\n",
    "    \"\"\"\n",
    "    Plots a histogram of the hate_speech_score with annotated lines at the thresholds\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to plot the histogram from\n",
    "        hate_threshold (float): The threshold for hate speech\n",
    "        supportive_threshold (float): The threshold for supportive speech\n",
    "\n",
    "    Returns:\n",
    "        None (plots the histogram)\n",
    "    \"\"\"\n",
    "\n",
    "    total_count = df.shape[0]\n",
    "    \n",
    "    # Calculate the amount of values below the supportive threshold\n",
    "    supportive_count = df[df['hate_speech_score'] < supportive_threshold].shape[0]\n",
    "\n",
    "    # Calculate the amount of values above the hate threshold\n",
    "    hate_count = df[df['hate_speech_score'] > hate_threshold].shape[0]\n",
    "\n",
    "    # Calculate the amount of values between the thresholds\n",
    "    neutral_count = df[(df['hate_speech_score'] >= supportive_threshold) & (df['hate_speech_score'] <= hate_threshold)].shape[0]\n",
    "\n",
    "    print(f\"Total count: {total_count}\")\n",
    "    print(f\"Supportive count: {supportive_count} ({supportive_count / total_count * 100:.2f}%)\")\n",
    "    print(f\"Hate count: {hate_count} ({hate_count / total_count * 100:.2f}%)\")\n",
    "    print(f\"Neutral count: {neutral_count} ({neutral_count / total_count * 100:.2f}%)\")\n",
    "\n",
    "    # Plot in histogram as well\n",
    "    plt.hist(df['hate_speech_score'], bins=100)\n",
    "    plt.axvline(hate_threshold, color='red', linestyle='--', label='Hate Speech Threshold')\n",
    "    plt.axvline(supportive_threshold, color='blue', linestyle='--', label='Supportive Threshold')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_target_columns_distribution(df: pd.DataFrame, target_cols: list[str]):\n",
    "    \"\"\"\n",
    "    Plots a barchart of the average value of the target columns for the hate speech observations\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to plot the distribution from\n",
    "        target_cols (list[str]): The columns to plot\n",
    "\n",
    "    Returns:\n",
    "        None (plots the distribution)\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter to count hate speech only\n",
    "    hate_speech_df = df[df['is_hate_speech'] == 1]\n",
    "\n",
    "    # Calculate the mean value of the target columns for the hate speech observations and sort them descending\n",
    "    mean_values = hate_speech_df[target_cols].mean()\n",
    "    mean_values = mean_values.sort_values(ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    mean_values.plot(kind='bar')\n",
    "    plt.title('Percentage of hate speech columns targeting a specific group')\n",
    "    plt.xlabel('Target Column')\n",
    "    plt.ylabel('Percentage of hate speech')\n",
    "    plt.show()\n",
    "\n",
    "def plot_target_columns_detailed(df: pd.DataFrame, target_cols: list[str], fig_size: tuple[int, int] = (15, 30), y_max: float = 0.35):\n",
    "    \"\"\"\n",
    "    Plots a bar chart for each of the target columns with detailed breakdowns of sub-groups\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to plot the distribution from\n",
    "        target_cols (list[str]): The columns to plot\n",
    "\n",
    "    Returns:\n",
    "        None (plots the distributions)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter to count hate speech only\n",
    "    hate_speech_df = df[df['is_hate_speech'] == 1]\n",
    "\n",
    "    # initalize axes to plot the 7 sub-groups in two rows of 3 and a last row of 1\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=2, figsize=fig_size)\n",
    "    # For each of the target columns, make a list of means of of hate speach targeting its sub-groups\n",
    "    for ax, col in zip(axes.flatten(), target_cols):\n",
    "\n",
    "        # Calculate the number of observations with a non-zero value for the target column\n",
    "        non_zero_count = hate_speech_df[col].value_counts()[1]\n",
    "\n",
    "        # identify all columns in the dataframe that starts with the target column name\n",
    "        sub_groups_column_names = [column for column in df.columns if column.startswith(col)]\n",
    "\n",
    "        # remove the original target column from the list\n",
    "        sub_groups_column_names.remove(col)\n",
    "        col_name = col.replace(\"target_\", \"\").title()\n",
    "\n",
    "        # calculate the mean values for each sub-group\n",
    "        sub_group_means = hate_speech_df[sub_groups_column_names].mean()\n",
    "        sub_group_means = sub_group_means.sort_values(ascending=False)\n",
    "\n",
    "        # remove the col string from the labels\n",
    "        labels = sub_group_means.index.str.replace((col + \"_\"), '')\n",
    "\n",
    "        # plot on the specific axis\n",
    "        sub_group_means.plot(kind='bar', ax=ax)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set_title(f'Percentage of hate speech targeting {col_name} by sub-group \\n(n={non_zero_count})')\n",
    "        ax.set_xlabel('Sub-group')\n",
    "        ax.set_ylabel('Percentage of hate speech')\n",
    "        ax.set_ylim(0, y_max)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.7)\n",
    "\n",
    "def inspect_for_cor(df, columns: list[str], plot_label: str, hate_speech_only: bool = True):\n",
    "\n",
    "    # filter for hate speech only if specified\n",
    "    if hate_speech_only:\n",
    "        df = df[df['is_hate_speech'] == 1]\n",
    "    \n",
    "    # Make a correlation matrix\n",
    "    corr_matrix = df[columns].corr()\n",
    "\n",
    "    # Plot the correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title(f'Correlation Heatmap for {plot_label}')\n",
    "    plt.show()\n",
    "\n",
    "    # Make a pairplot of the columns\n",
    "    sns.pairplot(df[columns])\n",
    "    plt.title(f'Pairplot for {plot_label}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22853d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_cols = [\n",
    "    'insult',\n",
    "    'humiliate',\n",
    "    'status',\n",
    "    'dehumanize',\n",
    "    'violence',\n",
    "    'genocide'\n",
    "]\n",
    "\n",
    "target_cols_lvl1_and_2 = [col for col in df_raw.columns if col.startswith('target_')]\n",
    "\n",
    "target_cols_lvl1 = [\n",
    "    'target_race',\n",
    "    'target_religion',\n",
    "    'target_origin',\n",
    "    'target_gender',\n",
    "    'target_sexuality',\n",
    "    'target_age',\n",
    "    'target_disability',    \n",
    "]\n",
    "\n",
    "cols_to_keep = [\n",
    "    'comment_id', \n",
    "    'text', \n",
    "    'hate_speech_score',\n",
    "    *aspect_cols,\n",
    "    *target_cols_lvl1_and_2\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37613ea6",
   "metadata": {},
   "source": [
    "### 2.1. Pre-filtering EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91890c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hate_speech_score_histogram(df_raw, HATE_SPEECH_THRESHOLD_UNNORMALIZED, SUPPORTIVE_THRESHOLD_UNNORMALIZED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f3881",
   "metadata": {},
   "source": [
    "### 2.2. Filtering & reformating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84c761",
   "metadata": {},
   "source": [
    "TODO: Make list of filtering actions for report \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows based on the 'comment_id' column\n",
    "duplicate_texts = df_raw[df_raw.duplicated(subset='comment_id', keep=False)]\n",
    "print(f\"Number of duplicate texts: {duplicate_texts.shape[0]}\")\n",
    "duplicate_texts[['comment_id', 'text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dc8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'comment_id' and count unique 'hate_speech_score' values per group\n",
    "score_check = df_raw.groupby(\"comment_id\")[\"hate_speech_score\"].nunique()\n",
    "\n",
    "# Find texts with more than one unique score\n",
    "inconsistent = score_check[score_check > 1]\n",
    "\n",
    "# Show how many inconsistencies there are\n",
    "print(f\"Number of 'comment_id' entries with inconsistent scores: {len(inconsistent)}\")\n",
    "\n",
    "# Optionally, view a few examples\n",
    "if not inconsistent.empty:\n",
    "    print(df_raw[df_raw[\"comment_id\"].isin(inconsistent.index)].sort_values(\"comment_id\").head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfe2e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select relevant columns\n",
    "df_relevant = df_raw[cols_to_keep].copy()\n",
    "\n",
    "# Step 2: Convert booleans to integers for mean calculation\n",
    "bool_cols = [col for col in df_relevant.columns if col.startswith('target_')]\n",
    "df_relevant[bool_cols] = df_relevant[bool_cols].astype(int)\n",
    "\n",
    "# Step 3: Group by 'comment_id' and 'text' to keep them in final output\n",
    "filtered_df = df_relevant.groupby(['comment_id', 'text']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bff2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = filtered_df['hate_speech_score'].min()  # e.g., -8.34\n",
    "max_score = filtered_df['hate_speech_score'].max()  # e.g., +6.3\n",
    "\n",
    "# replace hate speech core column with normalized column - [0, 1]\n",
    "filtered_df['hate_speech_score'] = (filtered_df['hate_speech_score'] - min_score) / (max_score - min_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f321de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the thresholds given by the dataset authors\n",
    "HATE_SPEECH_THRESHOLD_NORMALIZED = (0.5 - min_score) / (max_score - min_score)  \n",
    "SUPPORTIVE_THRESHOLD_NORMALIZED = (-1 - min_score) / (max_score - min_score)\n",
    "\n",
    "# Add binary columns for hate speech\n",
    "filtered_df.insert(filtered_df.columns.get_loc('hate_speech_score') + 1, 'is_hate_speech', (filtered_df['hate_speech_score'] > HATE_SPEECH_THRESHOLD_NORMALIZED).astype(int))\n",
    "\n",
    "# Print new normalized thresholds \n",
    "print(f\"Threshold for hate speech: {HATE_SPEECH_THRESHOLD_NORMALIZED:.3f} -> Equivalent to {0.5} on the original scale\")\n",
    "print(f\"Threshold for counter speech: {SUPPORTIVE_THRESHOLD_NORMALIZED:.3f} -> Equivalent to {-1} on the original scale\")\n",
    "print(f\"Between the two thresholds: {HATE_SPEECH_THRESHOLD_NORMALIZED:.3f} and {SUPPORTIVE_THRESHOLD_NORMALIZED:.3f} -> Equivalent to {0.5} and {-1} on the original scale which is the unambiguous region\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34e919",
   "metadata": {},
   "source": [
    "### 2.3. Post-filtering EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35914d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hate_speech_score_histogram(filtered_df, HATE_SPEECH_THRESHOLD_NORMALIZED, SUPPORTIVE_THRESHOLD_NORMALIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_target_columns_distribution(filtered_df, target_cols_lvl1)\n",
    "plot_target_columns_detailed(filtered_df, target_cols_lvl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_for_cor(filtered_df, target_cols_lvl1, 'Target Columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dc0b7c",
   "metadata": {},
   "source": [
    "# 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a48fc",
   "metadata": {},
   "source": [
    "### 3.0 Function & Class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99bbe327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_results(results: list, report: dict, model_name: str, metrics: list[str]):\n",
    "    \n",
    "    append_dict = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        append_dict[f'{metric}_hate'] = report['1'][metric]\n",
    "        append_dict[f'{metric}_non_hate'] = report['0'][metric]\n",
    "        append_dict[f'{metric}_macro_avg'] = report['macro avg'][metric]\n",
    "        append_dict[f'{metric}_weighted_avg'] = report['weighted avg'][metric]\n",
    "\n",
    "    append_dict['accuracy'] = report['accuracy']\n",
    "\n",
    "    results.append({\n",
    "        'model': model_name,\n",
    "        **append_dict\n",
    "    })\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"\n",
    "    Convert the part of speech tag to a format that WordNet lemmatizer can understand.\n",
    "    starts with 'J' for adjectives, 'V' for verbs, 'N' for nouns, and 'R' for adverbs.\n",
    "    Args:\n",
    "        tag (str): The part of speech tag.\n",
    "    Returns:\n",
    "        str: The WordNet part of speech tag.\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def is_ascii(token):\n",
    "    return all(ord(c) < 128 for c in token)\n",
    "\n",
    "def remove_repeated_chars(token, threshold=3):\n",
    "    return re.sub(r'(.)\\1{' + str(threshold) + r',}', r'\\1', token)\n",
    "\n",
    "def bow_preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by tokenizing, removing punctuation, stop words,\n",
    "    and lemmatizing the words.\n",
    "    Args:\n",
    "        text (str): The text to preprocess.\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    Example:\n",
    "        >>> preprocess(\"This is an EXAMPLE sentence!!!.\")\n",
    "        'example sentence'\n",
    "        \n",
    "    \"\"\"\n",
    "    tokens = tknzr.tokenize(text) # Tokenize the text\n",
    "    tokens = [word.lower() for word in tokens if word not in string.punctuation] # Remove punctuation\n",
    "    tokens = [word for word in tokens if word not in stop_words] # Remove stop words\n",
    "    tokens = [remove_repeated_chars(word) for word in tokens] # Remove repeated characters, ex: \"loooove\" -> \"love\"\n",
    "    tokens = [word for word in tokens if len(word) >= 2 and is_ascii(word)]  # Filter by length and ASCII\n",
    "    pos_tags = pos_tag(tokens) # Get part of speech tags\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags] # Lemmatize the words using the part of speech tags\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "def stream_batches(texts, labels, batch_size=32, max_length=256):\n",
    "    \"\"\"\n",
    "    Streams batches into SGD classifier\n",
    "\n",
    "    Args:\n",
    "        texts (pd.Series): The texts to stream\n",
    "        labels (pd.Series): The labels to stream\n",
    "        batch_size (int): The batch size\n",
    "        max_length (int): The maximum length of the texts\n",
    "    \n",
    "    Returns:\n",
    "        generator: A generator of batches\n",
    "    \"\"\"\n",
    "    n = len(texts)\n",
    "    n_batches = ceil(n / batch_size)\n",
    "\n",
    "    # Iterate over the batches to yield them one by one\n",
    "    for i in range(n_batches):\n",
    "        batch_texts = texts.iloc[i*batch_size:(i+1)*batch_size].tolist()\n",
    "        batch_labels = labels.iloc[i*batch_size:(i+1)*batch_size].values\n",
    "        \n",
    "        # dynamic padding + truncation\n",
    "        enc = bert_tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        yield enc[\"input_ids\"], enc[\"attention_mask\"], batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca5db4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWTextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.progress_apply(bow_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a64241",
   "metadata": {},
   "source": [
    "### 3.1. BoW + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for Bag of Words (BoW) representation\n",
    "bow_df = filtered_df.copy()\n",
    "\n",
    "# Releveant columns\n",
    "bow_cols = ['text', 'is_hate_speech']\n",
    "\n",
    "# Select relevant columns\n",
    "bow_df = bow_df[bow_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc79ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = bow_df['is_hate_speech'].value_counts()\n",
    "label_percentage = bow_df['is_hate_speech'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Label Counts:\")\n",
    "print(label_counts)\n",
    "print(\"\\nLabel Percentage Distribution:\")\n",
    "print(label_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36de96e",
   "metadata": {},
   "source": [
    "##### 3.1.1 Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb1c6f",
   "metadata": {},
   "source": [
    "TODO: Maybe remove preprocessing from pipepine to just have it done once for \"Part 2\" of Assignment 3 approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76203444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "for resource in ['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger_eng']:\n",
    "    nltk.download(resource, quiet=True)\n",
    "\n",
    "# Tokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51bf864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_pipeline = Pipeline([\n",
    "    ('preprocessor', BOWTextPreprocessor()),\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression(class_weight='balanced'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d9e567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bow_df['text']\n",
    "y = bow_df['is_hate_speech']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_RATIO, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264239dd",
   "metadata": {},
   "source": [
    "##### 3.1.2. Fit model to training set - BoW + LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to the training data\n",
    "bow_pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866b3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_bow = bow_pipeline.predict(X_test)\n",
    "\n",
    "# Generate the classification report\n",
    "report_bow = classification_report(y_test, y_pred_bow, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31455156",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_results(results, report_bow, 'BoW + LogisticRegression', EVALUATION_METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa679c",
   "metadata": {},
   "source": [
    "### 3.2 TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_tfidf = Pipeline([\n",
    "    ('preprocessor', BOWTextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', LogisticRegression(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_tfidf.fit(X_train, y_train)\n",
    "y_pred_tfidf = pipeline_tfidf.predict(X_test)\n",
    "\n",
    "report_tfidf = classification_report(y_test, y_pred_tfidf, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01aeb93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_results(results, report_tfidf, 'TF-IDF + LogisticRegression', EVALUATION_METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb087e",
   "metadata": {},
   "source": [
    "##### 3.2.2 Hyperparameter tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a76abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_tuned = Pipeline([\n",
    "    ('preprocessor', BOWTextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer()),  # or CountVectorizer()\n",
    "    ('classifier', LogisticRegression(class_weight='balanced', max_iter=1000))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1,1), (1,2)],\n",
    "    'vectorizer__max_features': [5000, 10000],\n",
    "    'vectorizer__max_df': [0.75, 0.9],\n",
    "    'classifier__C': [0.1, 1],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear']  # Needed to compare L1 and L2\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline_tuned,\n",
    "    param_grid,\n",
    "    cv=3, \n",
    "    scoring='f1', \n",
    "    verbose=2, \n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcadc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best cross-validated F1 score:\", grid_search.best_score_)\n",
    "\n",
    "# Use best model to predict test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "\n",
    "report_tuned = classification_report(y_test, y_pred_tuned, output_dict=True)\n",
    "\n",
    "append_results(results, report_tuned, 'Tuned BoW + LogisticRegression', EVALUATION_METRICS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb46646",
   "metadata": {},
   "source": [
    "### 3.3. BERT + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc28eb1",
   "metadata": {},
   "source": [
    "##### 3.3.1. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "23987cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "bert_tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "86c7a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df = filtered_df.copy()\n",
    "\n",
    "# Optional: Take a subset to ease computational load\n",
    "bert_df = bert_df.sample(frac=1)\n",
    "\n",
    "X = bert_df['text']\n",
    "y = bert_df['is_hate_speech']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_RATIO, random_state=42)\n",
    "\n",
    "batch_size = 16\n",
    "num_batches = ceil(len(X_train) / batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802848f2",
   "metadata": {},
   "source": [
    "Computing class weights for the balance_weight argument in partial fit (TODO: remove this and \"class weight\" argument from model instantiation if we decide to do under/oversampling instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "025733fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes must match what you’ll pass to partial_fit\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "# Compute: this returns an array [w_for_0, w_for_1]\n",
    "weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train,             # your full training-label vector\n",
    ")\n",
    "\n",
    "# Turn it into a dict: { class_label: weight, … }\n",
    "class_weight_dict = {c: w for c, w in zip(classes, weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0189b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SGD-based logistic regression\n",
    "clf = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", max_iter=1, warm_start=True, class_weight=class_weight_dict)\n",
    "classes = [0, 1]          \n",
    "first_batch = True\n",
    "\n",
    "train_iter = tqdm(\n",
    "    stream_batches(X_train, y_train, batch_size=batch_size),\n",
    "    total=num_batches,\n",
    "    desc=\"Training\"\n",
    ")\n",
    "\n",
    "\n",
    "# Loop (stream) through training data\n",
    "first_batch = True\n",
    "for input_ids, attn_mask, y_batch in train_iter:\n",
    "\n",
    "    # Get the features from the model\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids, attention_mask=attn_mask)\n",
    "\n",
    "    # pull out the [CLS] token embedding for each example\n",
    "    feats = out.last_hidden_state[:,0,:].cpu().numpy()\n",
    "    \n",
    "    if first_batch:\n",
    "        # partial_fit needs to see the 'classes' array at first call\n",
    "        clf.partial_fit(feats, y_batch, classes=classes)\n",
    "        first_batch = False\n",
    "    else:\n",
    "        clf.partial_fit(feats, y_batch)\n",
    "    \n",
    "\n",
    "# Evaluate on tast data, with streaming\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test data\n",
    "for input_ids, attn_mask, y_batch in stream_batches(X_test, y_test, batch_size=16):\n",
    "\n",
    "    # Get the features from the model\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids, attention_mask=attn_mask)\n",
    "    feats = out.last_hidden_state[:,0,:].cpu().numpy()\n",
    "\n",
    "    # Predict the labels\n",
    "    preds = clf.predict(feats)\n",
    "    \n",
    "    # Save\n",
    "    all_preds.append(preds)\n",
    "    all_labels.append(y_batch)\n",
    "\n",
    "# flatten and compute metric\n",
    "y_true = np.concatenate(all_labels)\n",
    "y_pred = np.concatenate(all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "609ef982",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_distilbert = classification_report(y_true, y_pred, output_dict=True)\n",
    "append_results(results, report_distilbert, 'BERT + LogisticRegression', EVALUATION_METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33600f62",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45021dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
